{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2c3c68",
   "metadata": {
    "id": "7a2c3c68"
   },
   "source": [
    "Como comentamos anteriormente, hemoss apartado 10,000 filas del dataset original y las hemos guardado en el csv 'last_10000_rows' para hacer predicciones. Para hacer el pre procesamiento, pasamos la funcion `dataset_processing( )` al csv 'last_10000_rows'. Las variables de la función `dataset_processing( )` están en el Notebook de Preprocessing y están basadas en los datos de Train. Trabajamos en local con Jupyter Notebooks porque así se guardan las variables asociadas al preprocesamiento y esto permite que la función `dataset_processing( )` pueda acceder a ellas. Aplicamos la funcion al dataset y, al terminar, quitamos la columna `is_fraud`, que luego habrá que añadirla y poblarla a la hora de hacer predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca4162c9",
   "metadata": {
    "id": "ca4162c9"
   },
   "outputs": [],
   "source": [
    "# Some basic Data Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# All rows and columns display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b273e61c",
   "metadata": {
    "id": "b273e61c"
   },
   "outputs": [],
   "source": [
    "df_rows = pd.read_csv('./data/last_10000_rows.csv', sep=',', decimal='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6ba1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82571696",
   "metadata": {
    "id": "82571696",
    "outputId": "cd6261b4-1c80-4f44-d6c6-59c456a309b0"
   },
   "outputs": [],
   "source": [
    "#pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe84c2d",
   "metadata": {},
   "source": [
    "Librerías que necesitamos en el Preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db058455",
   "metadata": {
    "id": "db058455",
    "outputId": "8c92c3d2-7a4d-4599-c282-57eb2ec4a059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in c:\\users\\usuario\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from geopy) (2.0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "!pip install geopy\n",
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05deca2",
   "metadata": {},
   "source": [
    "`dataset_processing( )` es la función que habíamos definido en el Notebook 4. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7436aa0",
   "metadata": {},
   "source": [
    "Pero primero tenemos que recuperar los valores de los Encoders que habíamos guardado, en previsión, en memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81d3264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r le_gender\n",
    "%store -r ohe_category\n",
    "%store -r targ_job\n",
    "%store -r ohe_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46672db",
   "metadata": {
    "id": "d46672db"
   },
   "outputs": [],
   "source": [
    "def dataset_processing (df):\n",
    "\n",
    "    # Dropping variables\n",
    "    df = df.drop(['state_abbr','trans_date_trans_time', 'time', 'date','zip','city', 'cc_num'], axis = 1)\n",
    "\n",
    "    # Weekdays into nums\n",
    "    df['day_of_week'] = df['day_of_week'].map({\n",
    "    'Monday': 0,\n",
    "    'Tuesday': 1,\n",
    "    'Wednesday': 2,\n",
    "    'Thursday': 3,\n",
    "    'Friday': 4,\n",
    "    'Saturday': 5,\n",
    "    'Sunday': 6 })\n",
    "\n",
    "    # is_holiday as num type\n",
    "    df['is_holiday'] = df['is_holiday'].astype(int)\n",
    "\n",
    "    # Label Encoder **previously defined in Train** for Gender\n",
    "    df[\"gender\"] = le_gender.transform(df[\"gender\"])\n",
    "\n",
    "    # One Hot encoder **previously defined in Train** for merchants categories\n",
    "    cat_encoded = ohe_category.transform(df[[\"category\"]])\n",
    "    merch_cats = ohe_category.get_feature_names_out()\n",
    "    df = pd.concat([df.drop(columns=[\"category\"]), pd.DataFrame(cat_encoded.toarray(),columns=merch_cats)], axis=1)\n",
    "\n",
    "    # Target encoder **previously defined in Train** for jobs\n",
    "    job_encoded = targ_job.transform(df[['job']])\n",
    "    te_job = targ_job.feature_names_in_\n",
    "    df = pd.concat([df.drop(columns=[\"job\"]), pd.DataFrame(job_encoded, columns=te_job)], axis=1)\n",
    "\n",
    "    # One-Hot-Encoding **previously defined in Train** for State\n",
    "    state_encoded = ohe_state.transform(df[[\"state\"]])\n",
    "    states = ohe_state.get_feature_names_out()\n",
    "    df = pd.concat([df.drop(columns=[\"state\"]), pd.DataFrame(state_encoded.toarray(), columns = states)], axis=1)\n",
    "\n",
    "    # Distance\n",
    "    df['merch_home_distance'] = df.apply(lambda row: geodesic((row['lat'], row['long']),\n",
    "                                                          (row['merch_lat'], row['merch_long'])).miles, axis=1) #In miles\n",
    "    # Online shopping transactions\n",
    "    online_columns = [col for col in df.columns if col.endswith('_net')]\n",
    "    df['is_online_shopping'] = df[online_columns].apply(lambda row: 1 if row.any() else 0, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e643f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows_processed = dataset_processing(df_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a7b83ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 88)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rows_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78efea7",
   "metadata": {
    "id": "c78efea7"
   },
   "outputs": [],
   "source": [
    "df_rows_processed = df_rows_processed.drop(['is_fraud'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e08be9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 87)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rows_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c92f1",
   "metadata": {},
   "source": [
    "## Guardamos este fichero en otro csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d31da6b5",
   "metadata": {
    "id": "d31da6b5"
   },
   "outputs": [],
   "source": [
    "df_rows_processed.to_csv('./data/last_10000_rows_proc.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
